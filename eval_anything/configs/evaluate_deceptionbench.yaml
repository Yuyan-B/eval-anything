# Copyright 2025 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================

# Evaluation Configuration
eval_cfgs:
  # Output directory
  output_dir: ../output
  # Cache directory; if not specified, cache will not be saved
  cache_dir: ../cache
  # Unique identifier for the current task
  task_uid: 111
  # Benchmark and task names; if task list is empty, all tasks will be evaluated
  benchmarks: {
    "DeceptionBench": ["DeceptionBench"],  # Run DeceptionBench evaluation
    }
  # Number of few-shot samples
  n_shot: {
    "DeceptionBench": 0,  # DeceptionBench does not use few-shot
    }
  # Chain of Thought
  cot: {
    "DeceptionBench": False,  # DeceptionBench has its own dual reasoning mechanism
    }
  # Visualization configuration
  visualization:
    enable: false  # Whether to enable visualization interface
    port: 8080     # Visualization port
    share: false   # Whether to share

# Model Configuration
model_cfgs:
  # Pretrained model unique identifier
  model_id: gpt-4o
  # Pretrained model name or path (supports local path or HuggingFace model name)
  model_name_or_path: gpt-4o
  # Model type ("LM" for language model, "MM" for multimodal model)
  model_type: "LM"
  # Chat template; if not specified, will use `tokenizer.apply_chat_template()`
  chat_template: null

# Inference Configuration
infer_cfgs:
  # Inference backend ("vllm", "transformers", "api", etc.)
  infer_backend: "api"
  # Whether to trust remote code
  trust_remote_code: True
  # Maximum input length
  model_max_length: 8192
  # Maximum number of newly generated tokens
  max_new_tokens: 8192
  # Number of outputs
  num_output: 1
  # Top-K sampling
  top_k: 50
  # Top-P sampling
  top_p: 0.95
  # Temperature parameter (0 means deterministic generation)
  temperature: 0.
  # Prompt log probabilities
  prompt_logprobs: 0
  # Log probabilities
  logprobs: 20
  # Whether to use beam search
  beam_search: False
  # Beam search size
  num_beams: null
  # Number of GPUs to use
  num_gpu: 4
  # Available GPU IDs; if not specified, will use the first num_gpu GPUs
  gpu_ids: [4,5,6,7]
  # GPU utilization
  gpu_utilization: 0.5
