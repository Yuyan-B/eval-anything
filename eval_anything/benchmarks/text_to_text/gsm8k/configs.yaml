# Copyright 2025 PKU-Alignment Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

dataset:
  name: gsm8k
  path: openai/gsm8k
  split: test
  size:
  modality: text-to-text
  fewshot_data_path: openai/gsm8k  # Optional: if not specified, few-shot functionality will be disabled
  fewshot_data_file: null
  fewshot_data_split: train
  cot_fewshot_data_path: benchmarks/cot_fewshot  # Optional: if not specified, Chain-of-Thought with few-shot functionality will be disabled
  cot_fewshot_data_file: gsm8k_main.json
  cot_fewshot_data_split: train
  max_shot: 5   # Optional: if not specified, the maximum number of few-shot examples will default to 0
  default_task_list: ["main"]
task:
  - name: main
    type: Dialogue  # Evaluation type specific to the dataset format; refer to eval_anything/dataloader/base_dataloader.py and eval_anything/dataloader/t2t_dataloader.py
    data_files: ["main/test*.parquet"]  # Optional: if not specified, the dataset will be loaded using the task name
    question_key: question  # Dataset key containing the input prompt/question
    answer_key: null  # Dataset key containing the candidate answer (if applicable)
    ground_truth_key: answer  # Dataset key containing the ground truth answer (if applicable)
    candidate_labels: null  # Optional: if not specified, the answer field will be used as candidate labels
    avalable_evaluate_tools: ["match_number_after_pattern", "match_number_after_answer", "match_last_number", "match_first_number"] # Available answer extractors (listed below); the first one serves as the default
  - name: multi_choice  # Example configuration for multiple choice tasks
    type: MultiChoice
    data_files: null
    question_key: question
    answer_key: choice
    ground_truth_key: answer
    candidate_labels: [A, B, C, D]
    avalable_evaluate_tools: ["match_number_after_pattern", "match_number_after_answer", "match_last_number", "match_first_number"]
answer_extractor:
  - name: match_number_after_pattern    # Method for extracting numerical answers from model responses; see eval_anything/evaluate_tools/t2t_tools.py
    function: regex_match_number
    args:
      additional_pattern: "#### {original_pattern}"
      match_index: -1   # Extract the last matching number
  - name: match_number_after_answer
    function: regex_match_number
    args:
      additional_pattern: "The answer is {original_pattern}."
      match_index: -1
  - name: match_last_number
    function: regex_match_number
    args:
      match_index: -1
  - name: match_first_number
    function: regex_match_number
    args:
      match_index: 0   # Extract the first matching number
metrics:    # Evaluation metrics to be computed; see eval_anything/evaluate_tools/metrics.py
  - name: accuracy
    function: accuracy
    args:
overall_metrics:
  - name: null
    function: null
    args:
      null
